{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have run the code using 'Bigram'. When I change the variable 'ngramRange' to 1, Code runs using 'Unigram'. You can see accuracy of Unigram at the end of the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, Firstly, I read the data line by line and stored in a Dataframe 'all data'.\n",
    "'all_data''s columns are topic, label, doc_identifier and text. You can see first 5 elements of all_data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_identifier</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241.txt</td>\n",
       "      <td>neg</td>\n",
       "      <td>i bought this album because i loved the title ...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>544.txt</td>\n",
       "      <td>neg</td>\n",
       "      <td>i was misled and thought i was buying the enti...</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>729.txt</td>\n",
       "      <td>neg</td>\n",
       "      <td>i have introduced many of my ell high school s...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>anything you purchase in the left behind serie...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>840.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>i loved these movies and i cant wiat for the t...</td>\n",
       "      <td>dvd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_identifier label                                               text  \\\n",
       "0        241.txt   neg  i bought this album because i loved the title ...   \n",
       "1        544.txt   neg  i was misled and thought i was buying the enti...   \n",
       "2        729.txt   neg  i have introduced many of my ell high school s...   \n",
       "3        278.txt   pos  anything you purchase in the left behind serie...   \n",
       "4        840.txt   pos  i loved these movies and i cant wiat for the t...   \n",
       "\n",
       "   topic  \n",
       "0  music  \n",
       "1  music  \n",
       "2  books  \n",
       "3  books  \n",
       "4    dvd  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "f = open(\"all_sentiment_shuffled.txt\",'r', encoding=\"utf8\")\n",
    "lines = f.readlines() # read the data line by line and store list lines\n",
    "\n",
    "topics = []\n",
    "labels = []\n",
    "doc_identifiers = []\n",
    "text = []\n",
    "\n",
    "for line in lines:\n",
    "    line.lower() # make all characters lowercase\n",
    "    splited_line = line.split() # split the line to spaces\n",
    "    topics.append(splited_line[0]) # first splited word is topic\n",
    "    labels.append(splited_line[1]) # second splited word is label of sentence\n",
    "    doc_identifiers.append(splited_line[2]) # third splited word is name of the file\n",
    "    for i in splited_line[3:]: # rest of them is sentence\n",
    "        if not i.isalpha(): # remove chars that is not in alphabet\n",
    "            splited_line.remove(i)\n",
    "    sentence = ' '.join(splited_line[3:]) # combine words again\n",
    "    text.append(sentence)\n",
    "\n",
    "# akeeps the all data into dataframe all_data\n",
    "dict = {\"topic\":topics, \"label\":labels, \"doc_identifier\":doc_identifiers, \"text\":text}\n",
    "all_data = pd.DataFrame(dict)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I splitted the data into 3 parts. Firstly, it is splitted as test and train data. Test/data = 1/5\n",
    "After that, I separeted datas based on whether it has a positive or negative label for easier implementation.\n",
    "I don't shuffle the data because the data is already shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into train and test.\n",
    "train, test = train_test_split(all_data, test_size=0.2, shuffle=False)\n",
    "train_pos = train[train['label'] == \"pos\"]\n",
    "train_neg = train[train['label'] == \"neg\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Bag of Words model for counting the number of times each word appears. My implementation of BOW is in the below.\n",
    "ngramRange is 2 now, So, my code is running as Bigram Model. I we change to 1, it would run as Unigram Model. I run with Unigram model before. Details are at the end of the file.\n",
    "\n",
    "English Stop Words are words that using commonly. For example, pronouns, prepositions, conjunctions...\n",
    "When I remove them, My code works more speedy as well as accuracy increases.\n",
    "\n",
    "Vectorizer lists all words in data that have a positive label. And it creates a matrix. Matrix keeps that each word's number of occurrence for each data sample.\n",
    "I create a new Dataframe through that matrix. New Dataframe's columns are vocabularies and its number of occurance from the matrix.\n",
    "\n",
    "!For Bigram Model, \"Vocabulary\" is a structure that occurred as getting together 2 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of occurrency</th>\n",
       "      <th>positive vocabularies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aa aaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>aa batteries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>aa cables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>aa forever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>aa rechargables</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number of occurrency positive vocabularies\n",
       "0                     1                aa aaa\n",
       "1                    21          aa batteries\n",
       "2                     1             aa cables\n",
       "3                     1            aa forever\n",
       "4                     1       aa rechargables"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "ngramRange = 2 # it can be change according to be unigram or bigram\n",
    "\n",
    "# remove stopwords and create unigram or bigram\n",
    "vectorizer_pos = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(ngramRange,ngramRange))\n",
    "x = vectorizer_pos.fit_transform(train_pos.loc[:,\"text\"])\n",
    "bow_matrix_pos = x.toarray() # create a bow matrix that keeps occurances of all words in positive data\n",
    "\n",
    "pos_vocabularies = vectorizer_pos.get_feature_names() # all words in positive data\n",
    "numberOfVoc_pos = np.sum(bow_matrix_pos,axis=0) # number of occurrency of vocabularies that have positive label\n",
    "dict_pos = {\"positive vocabularies\":pos_vocabularies, \"number of occurrency\":numberOfVoc_pos}\n",
    "pos_data = pd.DataFrame(dict_pos) # data that have words and their occurrency number\n",
    "pos_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same process occurs for data that have a negative label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative vocabularies</th>\n",
       "      <th>number of occurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa aaa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa batteries</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aa battery</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa batts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa cell</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  negative vocabularies  number of occurrency\n",
       "0                aa aaa                     2\n",
       "1          aa batteries                    10\n",
       "2            aa battery                     1\n",
       "3              aa batts                     1\n",
       "4               aa cell                     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords and create unigram or bigram\n",
    "vectorizer_neg = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(ngramRange,ngramRange))\n",
    "y = vectorizer_neg.fit_transform(train_neg.loc[:,\"text\"])\n",
    "bow_matrix_neg = y.toarray() # create a bow matrix that keeps occurrency of all words in negative data\n",
    "\n",
    "neg_vocabularies = vectorizer_neg.get_feature_names() # all words in negative data\n",
    "numberOfVoc_neg = np.sum(bow_matrix_neg,axis=0) # number of occurrency of vocabularies in negative label\n",
    "dict_neg = {\"negative vocabularies\":neg_vocabularies, \"number of occurrency\":numberOfVoc_neg}\n",
    "neg_data = pd.DataFrame(dict_neg) # data that have words and their occurrency number\n",
    "neg_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Calculating the accuracy for topics, I created Dataframe with the same method.\n",
    "\n",
    "Dataframe consist of topics and their number of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of occurrency</th>\n",
       "      <th>positive topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>812</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822</td>\n",
       "      <td>camera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>802</td>\n",
       "      <td>dvd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>794</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>803</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number of occurrency positive topic\n",
       "0                   812          books\n",
       "1                   822         camera\n",
       "2                   802            dvd\n",
       "3                   794         health\n",
       "4                   803          music"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeps topics of positive data and their occurrencies (for prediction according to topic)\n",
    "vectorizer_pos_topic = CountVectorizer()\n",
    "z = vectorizer_pos_topic.fit_transform(train_pos.loc[:,\"topic\"])\n",
    "bow_matrix_pos_topic = z.toarray()\n",
    "pos_topics = vectorizer_pos_topic.get_feature_names()\n",
    "numberOfTopic_pos = np.sum(bow_matrix_pos_topic,axis=0) # number of occurrencies of vocabularies in negative label\n",
    "dict_pos_topic = {\"positive topic\":pos_topics, \"number of occurrency\":numberOfTopic_pos}\n",
    "pos_topic_data = pd.DataFrame(dict_pos_topic)\n",
    "pos_topic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative topic</th>\n",
       "      <th>number of occurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>books</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>camera</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dvd</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>music</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  negative topic  number of occurrency\n",
       "0          books                   811\n",
       "1         camera                   793\n",
       "2            dvd                   787\n",
       "3         health                   772\n",
       "4          music                   775"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeps topics of negative data and their occurrencies (for prediction according to topic)\n",
    "vectorizer_neg_topic = CountVectorizer()\n",
    "t = vectorizer_neg_topic.fit_transform(train_neg.loc[:,\"topic\"])\n",
    "bow_matrix_neg_topic = t.toarray()\n",
    "neg_topics = vectorizer_neg_topic.get_feature_names()\n",
    "numberOfTopic_neg = np.sum(bow_matrix_neg_topic,axis=0) # number of occurances of vocabularies in negative label\n",
    "dict_neg_topic = {\"negative topic\":neg_topics, \"number of occurrency\":numberOfTopic_neg}\n",
    "neg_topic_data = pd.DataFrame(dict_neg_topic)\n",
    "neg_topic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Algorithm was implemeted here,\n",
    "\n",
    "For I dont want to calculate some constant values for each testing, I have calculated them out of the function.\n",
    "For example, \n",
    "\n",
    "'pos_in_all' is ratio of number of samples that have positive label to all data.\n",
    "\n",
    "'total_pos' is number of samples that have positive label.\n",
    "\n",
    "'denominator_pos' is used for Laplace Smoothing. If a test word does not exist in the train data, the number of all positive and negative words increases by 1. So, we assume that we have one sample of this test data. Thus, the number of total positive words increases by the length of the positive words list.\n",
    "\n",
    "After these calculation, Naive Bayes algorithm is ready to run. I used log probability and Laplace Smoothing when I was implementing the Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# variables that will be need to be able to calculate naive-bayes\n",
    "\n",
    "# ratio of positive data to all\n",
    "pos_in_all = sum(numberOfVoc_pos)/(sum(numberOfVoc_pos)+sum(numberOfVoc_neg))\n",
    "# ratio of negative data to all\n",
    "neg_in_all = sum(numberOfVoc_neg)/(sum(numberOfVoc_pos)+sum(numberOfVoc_neg))\n",
    "# total number of positive data\n",
    "total_pos = sum(numberOfVoc_pos)\n",
    "# total number of negative data\n",
    "total_neg = sum(numberOfVoc_neg)\n",
    "# For Laplace Smoothing, denominators of text and topics\n",
    "denominator_pos = total_pos+len(pos_vocabularies)\n",
    "denominator_neg = total_neg+len(neg_vocabularies)\n",
    "denominator_pos_topics = total_pos+len(pos_topics)\n",
    "denominator_neg_topics = total_neg+len(neg_topics)\n",
    "\n",
    "# Naive-Bayes Algorithm Implementation\n",
    "def naive_bayes(test_vocs,isTopic,isText):\n",
    "\n",
    "    pos_likelihood = 0\n",
    "    neg_likelihood = 0\n",
    "\n",
    "    if isText == 1: # predict according to text\n",
    "        for voc in test_vocs:\n",
    "            if (voc not in pos_vocabularies) or (voc not in neg_vocabularies):\n",
    "                probability_in_pos = math.log(1/denominator_pos)\n",
    "                probability_in_neg = math.log(1/denominator_neg)\n",
    "            else:\n",
    "                # take occurrency of word that the same as test according to its label\n",
    "                probability_in_pos = math.log(pos_data[pos_data[\"positive vocabularies\"] == voc][\"number of occurrency\"] / total_pos)\n",
    "                probability_in_neg = math.log(neg_data[neg_data[\"negative vocabularies\"] == voc][\"number of occurrency\"] / total_neg)\n",
    "\n",
    "            neg_likelihood += probability_in_neg\n",
    "            pos_likelihood += probability_in_pos\n",
    "\n",
    "    elif isTopic == 1: # predict according to topic\n",
    "        for voc in test_vocs:\n",
    "            if (voc not in pos_topics) or (voc not in neg_topics):\n",
    "                probability_in_pos = math.log(1/denominator_pos_topics)\n",
    "                probability_in_neg = math.log(1/denominator_neg_topics)\n",
    "            else:\n",
    "                probability_in_pos = math.log(pos_topic_data[pos_topic_data[\"positive topic\"] == voc][\"number of occurrency\"] / total_pos)\n",
    "                probability_in_neg = math.log(neg_topic_data[neg_topic_data[\"negative topic\"] == voc][\"number of occurrency\"] / total_neg)\n",
    "\n",
    "            neg_likelihood += probability_in_neg\n",
    "            pos_likelihood += probability_in_pos\n",
    "\n",
    "    final_pos_prob = pos_likelihood+math.log(pos_in_all)\n",
    "    final_neg_prob = neg_likelihood+math.log(neg_in_all)\n",
    "\n",
    "    if final_pos_prob>final_neg_prob:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I run the Naive Bayes, If I want to algorithm predicts sentiment accoring to words, Paremeter 'isText' becomes 1, If I want to algorithm predicts sentiment according to topic, parameter 'isTopic' becomes 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the remove stopwords from text, 1 word remains in some text. This causes an error for bigram model. To be able to handle it, I wrote a try-except part. If lenght of the text is smaller than 2, this document goes to misclassifieds, directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Misclassifieds:\n",
      "444.txt : pos the series just keeps on getting better the bob newhart show was one of the best comedies ever on television the other funny show was newhart which i cannot understand why it has not been released on dvd the whole cast just clicks i wish shows like this were still on television if you enjoy a good laugh then buy this and the other sets\n",
      "460.txt : pos this is one of the better historical dramas from the many are stagey and slow and while this production of madame bovary is very much constrained to stay indoors this works to advantage for this story of a woman who feels so trapped by her life and her world emma bovary is not a very sympathetic creature she is married to a man who loves her with all his heart who tries to give her everything she wants is willing to ruin himself to make her happy and she still cheats on him and remains miserable in short she is selfish and inconsiderate as ugly inside as she is pretty on the outside francesca annis plays emma she is indeed very beautiful as an actress she often comes across as bright and hard flirty and flighty but cold and self satisfied these qualities of course are perfect for this character here she seems to be more in love with the idea of being in love than actually loving almost anything to break up the dismal tedium of her life and her disgust with everyone and everything in it it is very hard for a modern audience to feel sorry for her her daily trial does not include housework or drudgery she has maids for that she is bored because she is useless she is useless because she is too lazy to seek something meaningful to do she wants life to be a party and resents it when it is not tom conti plays her devoted husband who is completely devoid of ambition in work or society as an actor conti often seems to have just woken from a nap and this dampness is just right for bovary he too is very lazy in his way but his seems to stem from ignorance when contrasted with emma willfulness her husband seems the infinitely better of the two conti is really fine here as a man completely out of his depth with this racehorse of a wife in much the same way while annis is briskly carrying every scene conti just quietly steals every one of them a perfect pairing for these roles all the actors here are top notch and the casting a bit off beat which adds to the interest the costumes especially emmas are a luxurious parade of overindulgence absolutely beautiful while we as an audience enjoy the parade of finery we can also see how this wardrobe would drive even the richest man into the poorhouse the production is topped off with a novel and lovely score of predominantly piano compositions pretty and liltling but melancholy and dissonant do let this one get lost in the shuffle it is worth seeing\n",
      "519.txt : pos a wonderful wonderful the book was written some year ago mary eliza rogers takes you into the intimacies of daily life in palestine in the as if it was occurring today she writes from her heart with honesty integrity and a clear mind and although written at a time of victorian prejudicies and colonialism she writes without bias or judgement from her beautiful and colourful descriptions one can envisage the holy land as it was before undergoing the process of modernisation and change for anyone who has any attachment to this land it is a truly wonderful and personal experience to read this book\n",
      "841.txt : neg this camera has a very poor lens at zoom the barrel spherical distortion is enormous try taking a picture of a tiled wall image quality is also when a picture is viewed at scaled down on a computer monitor the quality is ok color fidelity is next to nothing when using flash and auto white balance\n",
      "1.txt : pos this is a wonderful album that evokes memories of the folk boom yet contains original songs i was amazed at the fantastic harmonies and musical arrangements anyone who loves the movie a mighty wind and who loves folk music will fall in love with this album i know i did\n",
      "925.txt : pos i have been a fan since valotte and i like the other reviewers here ca believe why this cd did receive the airplay it deserves there must have been some politics in there with sean cd coming out on the same day yoko it is one of those cds that you just leave on and you do feel like you have to go and skip over songs that you do like i love every song especially day after day i should have known i do want to know and my fav right now cold i hope that what i have heard is true that he is working on his latest now ca wait buy this you wo regret it\n",
      "904.txt : pos goo\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "false = 0\n",
    "misclassifieds = [] # list of 5 misclassified datas\n",
    "for index in test.index:\n",
    "    vectorizer_test = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(ngramRange,ngramRange))\n",
    "    try:\n",
    "        u = vectorizer_test.fit_transform([test.loc[index,\"text\"]])\n",
    "    except ValueError:\n",
    "        misclassifieds.append(index)\n",
    "        continue\n",
    "    words = vectorizer_test.get_feature_names()\n",
    "    bow = u.toarray()\n",
    "    for i in range(len(bow[0])):\n",
    "        if bow[0][i] > 1:\n",
    "            for j in range(bow[0][i]-1):\n",
    "                words.append(words[i])\n",
    "\n",
    "    prediction = naive_bayes(words,0,1) # predict according to text\n",
    "    if prediction == test.loc[index,\"label\"]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        false += 1\n",
    "        if len(misclassifieds) < 6:\n",
    "            misclassifieds.append(index)\n",
    "\n",
    "print(\"Some Misclassifieds:\")\n",
    "for i in misclassifieds:\n",
    "    print(test.loc[i,\"doc_identifier\"],\":\",test.loc[i,\"label\"],test.loc[i,\"text\"])\n",
    "\n",
    "accuracy = 100*correct/(correct+false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example in 351.txt, 'Not' and 'uncommon' are negatively effective words but when they use together, it has positive meaning. This stuation can create complication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy using Bigram Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using Bigram:  65.23929471032746\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of prediction using Bigram: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy using Unigram Model: %80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My implementation works better in Unigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction from topic:  48.38438942509442\n"
     ]
    }
   ],
   "source": [
    "# Prediction From Topic\n",
    "\n",
    "correct = 0\n",
    "false = 0\n",
    "\n",
    "for index in test.index:\n",
    "    words = [test.loc[index,\"topic\"]]\n",
    "\n",
    "    prediction = naive_bayes(words,1,0)\n",
    "    if prediction == test.loc[index,\"label\"]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        false += 1\n",
    "\n",
    "accuracy = 100*correct/(correct+false)\n",
    "print(\"Accuracy of prediction from topic: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that prediction the sentiment using topic of document is not a good way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyzing effect of the stopwords\n",
    "\n",
    "Removing stop words speed up as well as improve the accuracy because stopwords are existing both of the documents that has label negative and positive. So it is usefull but in bigram model, when I remove the stopwords from document, many prepositions are deleted. So, juxtaposed words became meaningless. This can be reason of bigram model has less accuracy from unigram has."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
